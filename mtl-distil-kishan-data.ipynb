{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8035171,"sourceType":"datasetVersion","datasetId":4736610}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Teacher","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport os\nfrom skimage import io\nfrom skimage.transform import resize\nimport scipy.io as spio\nimport torch\nfrom torch import nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nimport sys\nimport torchvision.models as models\nimport torch.nn.functional as F\nimport pandas as pd\nimport pickle\nimport os\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, r2_score, mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data.dataset import Subset\ntorch.manual_seed(1234) ","metadata":{"execution":{"iopub.status.busy":"2024-04-19T09:46:13.057681Z","iopub.execute_input":"2024-04-19T09:46:13.058488Z","iopub.status.idle":"2024-04-19T09:46:22.729793Z","shell.execute_reply.started":"2024-04-19T09:46:13.058460Z","shell.execute_reply":"2024-04-19T09:46:22.728851Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7abb13cf6510>"},"metadata":{}}]},{"cell_type":"code","source":"epochs = np.arange(1, 51)\nglass_accuracy_test = []\ngender_accuracy_test = []\ntesting_error_pose = []\nglass_accuracy_train = []\ngender_accuracy_train = []\ntraining_error_pose = []\nval_error_pose = []\nglass_accuracy_val = []\ngender_accuracy_val = []\nbest_val_loss = float('inf')\nbest_model_state = None\nearly_stopping_rounds = 5\npatience = 0\nNumEpochs = 50\ntorch.manual_seed(1234) \n\nnum_classes_task1 = 2 \nnum_classes_task2 = 2 \nnum_classes_task3 = 3 ","metadata":{"execution":{"iopub.status.busy":"2024-04-19T09:46:32.798952Z","iopub.execute_input":"2024-04-19T09:46:32.799883Z","iopub.status.idle":"2024-04-19T09:46:32.806640Z","shell.execute_reply.started":"2024-04-19T09:46:32.799849Z","shell.execute_reply":"2024-04-19T09:46:32.805884Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import torch\nprint(torch.cuda.is_available())\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2024-04-19T09:46:35.365780Z","iopub.execute_input":"2024-04-19T09:46:35.366170Z","iopub.status.idle":"2024-04-19T09:46:35.426394Z","shell.execute_reply.started":"2024-04-19T09:46:35.366143Z","shell.execute_reply":"2024-04-19T09:46:35.425221Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"True\n","output_type":"stream"}]},{"cell_type":"code","source":"img_size = [224, 224]\nstartFrom=0\n# NumEpochs = 30\nbatch_size = 100\nthreadsTraining=1  \ndataset_path='/kaggle/input/mtl-aflw-data'","metadata":{"execution":{"iopub.status.busy":"2024-04-19T09:46:42.612153Z","iopub.execute_input":"2024-04-19T09:46:42.612512Z","iopub.status.idle":"2024-04-19T09:46:42.617369Z","shell.execute_reply.started":"2024-04-19T09:46:42.612485Z","shell.execute_reply":"2024-04-19T09:46:42.616355Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"imagesPath='/kaggle/input/mtl-aflw-data/cropped_resized_AFLW/images'\nmat=spio.loadmat('/kaggle/input/mtl-aflw-data/cropped_resized_AFLW/matrices/dataAFLW.mat')\nmat2=spio.loadmat('/kaggle/input/mtl-aflw-data/cropped_resized_AFLW/matrices/protocolAFLW.mat')\n\nPresenceGender=mat2['TrainMask_Gender']\nPresenceGlasses=mat2['TrainMask_Glasses']\nPresencePose=mat2['TrainMask_Pose']\nLabelPresenceMat=np.concatenate((PresenceGender, PresenceGlasses, PresencePose))\nLabelPresenceMat=np.transpose(LabelPresenceMat)\nTrainingSampleCount_taskWise=np.sum(LabelPresenceMat,axis=0)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-19T09:46:45.211481Z","iopub.execute_input":"2024-04-19T09:46:45.212002Z","iopub.status.idle":"2024-04-19T09:46:45.451994Z","shell.execute_reply.started":"2024-04-19T09:46:45.211966Z","shell.execute_reply":"2024-04-19T09:46:45.450926Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"elementsInTest = int(PresenceGender.shape[1]*0.3)\nTrainingSampleCount = int(PresenceGender.shape[1]*0.7)","metadata":{"execution":{"iopub.status.busy":"2024-04-19T09:46:49.857226Z","iopub.execute_input":"2024-04-19T09:46:49.857865Z","iopub.status.idle":"2024-04-19T09:46:49.862426Z","shell.execute_reply.started":"2024-04-19T09:46:49.857835Z","shell.execute_reply":"2024-04-19T09:46:49.861463Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def loadCompleteDataset(startFrom, dataset_path,TrainingSampleCount):\n    ArrayFaceID=mat['ArrayFaceID']\n    ArrayGender_oneHot=mat['ArrayGender_oneHot']\n    ArrayGlasses_oneHot=mat['ArrayGlasses_oneHot']\n    ArraysPose=mat['ArraysPose']\n    \n    delMask=np.arange(startFrom,startFrom+elementsInTest)\n    \n    TestArrayFaceID = ArrayFaceID[startFrom:startFrom+elementsInTest:1,:]\n    TrainArrayFaceID = ArrayFaceID\n    TrainArrayFaceID = np.delete(TrainArrayFaceID,delMask,axis=0)\n    TestStrNames = [str(i) for i in TestArrayFaceID]\n    TestStrNames = [i[1:6] for i in TestStrNames]\n    TrainStrNames = [str(i) for i in TrainArrayFaceID]\n    TrainStrNames = [i[1:6] for i in TrainStrNames]\n    \n    TestArrayGender_oneHot = ArrayGender_oneHot[startFrom:startFrom+elementsInTest:1,:]\n    TrainArrayGender_oneHot = ArrayGender_oneHot\n    TrainArrayGender_oneHot = np.delete(TrainArrayGender_oneHot,delMask,axis=0)\n    \n    TestArrayGlasses_oneHot = ArrayGlasses_oneHot[startFrom:startFrom+elementsInTest:1,:]\n    TrainArrayGlasses_oneHot = ArrayGlasses_oneHot\n    TrainArrayGlasses_oneHot = np.delete(TrainArrayGlasses_oneHot,delMask,axis=0)\n    \n    TestArraysPose = ArraysPose[startFrom:startFrom+elementsInTest:1,:]\n    TrainArraysPose = ArraysPose\n    TrainArraysPose = np.delete(TrainArraysPose,delMask,axis=0)\n\n    dataset_AFLW = datasetClass_AFLW(TrainStrNames, TrainArrayGender_oneHot, TrainArrayGlasses_oneHot, TrainArraysPose, LabelPresenceMat, imagesPath, TrainingSampleCount)\n    dataset_AFLW_Test = datasetClass_AFLW_test(TestStrNames, TestArrayGender_oneHot, TestArrayGlasses_oneHot, TestArraysPose, imagesPath)\n    return dataset_AFLW, TrainingSampleCount_taskWise.astype(float), dataset_AFLW_Test, elementsInTest\n","metadata":{"execution":{"iopub.status.busy":"2024-04-19T09:46:54.337513Z","iopub.execute_input":"2024-04-19T09:46:54.337885Z","iopub.status.idle":"2024-04-19T09:46:54.348296Z","shell.execute_reply.started":"2024-04-19T09:46:54.337858Z","shell.execute_reply":"2024-04-19T09:46:54.347269Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class datasetClass_AFLW(Dataset):\n    def __init__(self, ArrayFaceID, ArrayGender_oneHot, ArrayGlasses_oneHot, ArraysPose, LabelPresenceMat, imagesPath, TrainingSampleCount):\n        self.ArrayFaceID=ArrayFaceID\n        self.ArrayGender_oneHot=torch.from_numpy(ArrayGender_oneHot)\n        self.ArrayGlasses_oneHot=torch.from_numpy(ArrayGlasses_oneHot)\n        self.ArraysPose=torch.from_numpy(ArraysPose)\n        self.imagePath=imagesPath\n        self.CountSamples = TrainingSampleCount#len(self.ArrayFaceID)\n        self.LabelPresenceMat=LabelPresenceMat\n    \n    def __getitem__(self, index):\n        imageName=self.ArrayFaceID[index]\n        ## Original Clean Image [output]\n        im_path = os.path.join(self.imagePath,imageName+'.png')\n        img = io.imread(im_path)\n        #following line normalizes image to [0, 1]\n        img = resize(img, (img_size[0], img_size[1]))\n        img = img.transpose(2, 0, 1)\n        \n        TaskwiseLabelPresence=self.LabelPresenceMat[index]\n        \n        label_gender = self.ArrayGender_oneHot[index]\n        label_glass = self.ArrayGlasses_oneHot[index]\n        label_pose = self.ArraysPose[index]\n        return  img, label_gender, label_glass, label_pose, imageName, TaskwiseLabelPresence\n        \n    def __len__(self):\n        return self.CountSamples\n","metadata":{"execution":{"iopub.status.busy":"2024-04-19T09:47:01.567764Z","iopub.execute_input":"2024-04-19T09:47:01.568642Z","iopub.status.idle":"2024-04-19T09:47:01.577065Z","shell.execute_reply.started":"2024-04-19T09:47:01.568612Z","shell.execute_reply":"2024-04-19T09:47:01.576161Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class datasetClass_AFLW_test(Dataset):\n    def __init__(self, ArrayFaceID, ArrayGender_oneHot, ArrayGlasses_oneHot, ArraysPose, imagesPath):\n        self.ArrayFaceID=ArrayFaceID\n        self.ArrayGender_oneHot=torch.from_numpy(ArrayGender_oneHot)\n        self.ArrayGlasses_oneHot=torch.from_numpy(ArrayGlasses_oneHot)\n        self.ArraysPose=torch.from_numpy(ArraysPose)\n        self.imagePath=imagesPath\n        self.CountSamples = elementsInTest\n    \n    def __getitem__(self, index):\n        imageName=self.ArrayFaceID[index]\n        ## Original Clean Image [output]\n        im_path = os.path.join(self.imagePath,imageName+'.png')\n        img = io.imread(im_path)\n        #following line normalizes image to [0, 1]\n        img = resize(img, (img_size[0], img_size[1]))\n        img = img.transpose(2, 0, 1)\n        \n        label_gender = self.ArrayGender_oneHot[index]\n        label_glass = self.ArrayGlasses_oneHot[index]\n        label_pose = self.ArraysPose[index]\n        return img, label_gender, label_glass, label_pose, imageName\n        \n    def __len__(self):\n        return self.CountSamples\n\n    \n","metadata":{"execution":{"iopub.status.busy":"2024-04-19T09:47:07.103533Z","iopub.execute_input":"2024-04-19T09:47:07.103936Z","iopub.status.idle":"2024-04-19T09:47:07.112511Z","shell.execute_reply.started":"2024-04-19T09:47:07.103907Z","shell.execute_reply":"2024-04-19T09:47:07.111526Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(1234) \ndataset_AFLW, TrainingSampleCount_taskWise, dataset_AFLW_Test, elementsInTest = loadCompleteDataset(startFrom, dataset_path, TrainingSampleCount)","metadata":{"execution":{"iopub.status.busy":"2024-04-19T09:47:09.243448Z","iopub.execute_input":"2024-04-19T09:47:09.243826Z","iopub.status.idle":"2024-04-19T09:47:10.158540Z","shell.execute_reply.started":"2024-04-19T09:47:09.243799Z","shell.execute_reply":"2024-04-19T09:47:10.157794Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(1234) \ntrain_indices, val_indices = train_test_split(np.arange(len(dataset_AFLW)), test_size=0.3, random_state=42)\ntrain_dataloader = DataLoader(Subset(dataset_AFLW, train_indices), batch_size=batch_size, shuffle=True, num_workers=threadsTraining, drop_last=True)\nval_dataloader = DataLoader(Subset(dataset_AFLW, val_indices), batch_size=batch_size, shuffle=False, num_workers=threadsTraining, drop_last=True)\ntest_dataloader = DataLoader(dataset_AFLW_Test, shuffle=False, num_workers=threadsTraining, batch_size=batch_size,drop_last=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-19T09:47:13.386689Z","iopub.execute_input":"2024-04-19T09:47:13.387589Z","iopub.status.idle":"2024-04-19T09:47:13.401080Z","shell.execute_reply.started":"2024-04-19T09:47:13.387556Z","shell.execute_reply":"2024-04-19T09:47:13.400258Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"**TEACHER_MTL**","metadata":{}},{"cell_type":"code","source":"class SharedNetwork(nn.Module): \n    def __init__(self):\n        super(SharedNetwork, self).__init__()\n        resnet = models.resnet18(pretrained=True)\n        self.shared_network = nn.Sequential(*list(resnet.children())[:-2])\n        self.gap = nn.Sequential(nn.AdaptiveAvgPool2d(1))\n\n    def forward(self, x):\n        shared_embedding = self.gap(self.shared_network(x))\n        return shared_embedding\n\nclass MultiTaskNetwork(nn.Module):\n    def __init__(self, num_classes_task1, num_classes_task2, num_classes_task3):\n        super(MultiTaskNetwork, self).__init__()\n        self.shared_net = SharedNetwork().to(device)\n        self.task1_head = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.ReLU(inplace=True),\n            nn.Linear(256, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, num_classes_task1),\n            nn.Softmax()\n        ).to(device)\n            \n        self.task2_head = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.ReLU(inplace=True),\n            nn.Linear(256, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, num_classes_task2),\n            nn.Softmax()\n        ).to(device)\n\n        self.task3_head = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.ReLU(inplace=True),\n            nn.Linear(256, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, num_classes_task3),\n            nn.ReLU(inplace=True)\n        ).to(device)\n\n    def forward(self, x):\n        shared_embedding = self.shared_net(x)\n        task1_output = self.task1_head(shared_embedding.view(shared_embedding.size(0), -1))\n        task2_output = self.task2_head(shared_embedding.view(shared_embedding.size(0), -1))\n        task3_output = self.task3_head(shared_embedding.view(shared_embedding.size(0), -1))\n        return task1_output, task2_output, task3_output\n\nclass myMSElossPose(torch.nn.Module):\n    def __init__(self, batch_size):\n        super(myMSElossPose, self).__init__()\n        self.batch_size = batch_size\n\n    def forward(self, predicted, target):\n        BatchSampleCount=predicted.shape[0]\n        error = torch.sub(predicted,target)\n        squaredError = torch.mul(error, error)\n        allSum = torch.sum(squaredError)\n        FinalLoss = torch.div(allSum, 3*BatchSampleCount)\n        return FinalLoss\n","metadata":{"execution":{"iopub.status.busy":"2024-04-19T09:47:15.627569Z","iopub.execute_input":"2024-04-19T09:47:15.627944Z","iopub.status.idle":"2024-04-19T09:47:15.643414Z","shell.execute_reply.started":"2024-04-19T09:47:15.627915Z","shell.execute_reply":"2024-04-19T09:47:15.642517Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def calculate_classification_metrics(labels, predictions):\n    predictions_index = np.argmax(predictions, axis=1)\n    labels_index = np.argmax(labels, axis=1)\n    true_1 = []\n    pred_1 = []\n    for i in range(len(predictions)):\n        true_1.append(labels[i][0])\n        if(predictions[i][0] >=0.5):\n            pred_1.append(1)\n        else:\n            pred_1.append(0)\n\n    true_2 = []\n    pred_2 = []\n    for i in range(len(predictions)):\n        true_2.append(labels[i][1])\n        if(predictions[i][1] >=0.5):\n            pred_2.append(1)\n        else:\n            pred_2.append(0)\n\n    precision_1 = precision_score(true_1, pred_1)\n    recall_1 = recall_score(true_1, pred_1)\n    accuracy_1 = accuracy_score(true_1, pred_1)\n    f1_1 = f1_score(true_1, pred_1)\n\n    precision_2 = precision_score(true_2, pred_2)\n    recall_2 = recall_score(true_2, pred_2)\n    accuracy_2 = accuracy_score(true_2, pred_2)\n    f1_2 = f1_score(true_2, pred_2)\n\n\n    print('For Class 0:')\n    print(f\"Precision_0: {precision_1:.4f}, Recall_0: {recall_1:.4f}, Accuracy_0: {accuracy_1:.4f}, F1 Score_0: {f1_1:.4f}\")\n    print('For Class 1:')\n    print(f\"Precision_1: {precision_2:.4f}, Recall_1: {recall_2:.4f}, Accuracy_1: {accuracy_2:.4f}, F1 Score_1: {f1_2:.4f}\")\n    return accuracy_1\n","metadata":{"execution":{"iopub.status.busy":"2024-04-19T09:47:22.735190Z","iopub.execute_input":"2024-04-19T09:47:22.735556Z","iopub.status.idle":"2024-04-19T09:47:22.745572Z","shell.execute_reply.started":"2024-04-19T09:47:22.735527Z","shell.execute_reply":"2024-04-19T09:47:22.744651Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"multi_task_net = MultiTaskNetwork(num_classes_task1, num_classes_task2, num_classes_task3).to(device)\n\nfor m in multi_task_net.modules():\n    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n        nn.init.xavier_normal_(m.weight)\n\n\ncriterion_task1 = nn.BCELoss().to(device)\ncriterion_task2 = nn.BCELoss().to(device)\ncriterion_task3 = myMSElossPose(batch_size).to(device)\n\n\noptimizer = optim.SGD(multi_task_net.parameters(), lr=0.001, momentum=0.9)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-19T09:47:24.903500Z","iopub.execute_input":"2024-04-19T09:47:24.903866Z","iopub.status.idle":"2024-04-19T09:47:25.791235Z","shell.execute_reply.started":"2024-04-19T09:47:24.903839Z","shell.execute_reply":"2024-04-19T09:47:25.790445Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 166MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"def training_result(epoch, multi_task_net, train_dataloader):\n    criterion_task3 = myMSElossPose(batch_size).to(device)\n    multi_task_net.eval()\n\n    y_true_glass = []\n    y_pred_glass = []\n    y_true_gender = []\n    y_pred_gender = []\n    y_true_pose = []\n    y_pred_pose = []\n    R_te_errorPose=0\n    te_countPose=0\n\n    # Iterate over the test dataset\n    with torch.no_grad():\n        for i, data in enumerate(train_dataloader, 0):\n            img, label_gender, label_glass, label_pose, imageName, TaskwiseLabelPresence = data\n            img = img.to(device)\n            label_gender = label_gender.to(device)\n            label_glass = label_glass.to(device)\n            label_pose = label_pose.to(device)\n            TaskwiseLabelPresence = TaskwiseLabelPresence.numpy()\n            img, label_gender, label_glass, label_pose = img.float(), label_gender.float(), label_glass.float(), label_pose.float()\n\n            # Make predictions\n            FC_output_gender, FC_output_glasses, FC_output_angle = multi_task_net(img)\n\n            gender_IndSelect = np.where(TaskwiseLabelPresence[:, 0] == 1)\n            glasses_IndSelect = np.where(TaskwiseLabelPresence[:, 1] == 1)\n            pose_IndSelect = np.where(TaskwiseLabelPresence[:, 2] == 1)\n\n            # Calculate task-wise metrics\n            if gender_IndSelect[0].shape[0] != 0:\n                y_true_gender.append(label_gender)\n                y_pred_gender.append(FC_output_gender)\n\n            if glasses_IndSelect[0].shape[0] != 0:\n                y_true_glass.append(label_glass)\n                y_pred_glass.append(FC_output_glasses)\n\n            if pose_IndSelect[0].shape[0] != 0:\n                y_true_pose.append(label_pose)\n                y_pred_pose.append(FC_output_angle)\n                loss3 = criterion_task3(FC_output_angle, label_pose)\n                R_te_errorPose += loss3.item()\n                te_countPose = te_countPose+1\n\n   \n    y_true_gender = np.concatenate([t.cpu().numpy() for t in y_true_gender], axis=0)\n    y_pred_gender = np.concatenate([t.cpu().numpy() for t in y_pred_gender], axis=0)\n    y_true_glass = np.concatenate([t.cpu().numpy() for t in y_true_glass], axis=0)\n    y_pred_glass = np.concatenate([t.cpu().numpy() for t in y_pred_glass], axis=0)\n    y_true_pose = np.concatenate([t.cpu().numpy() for t in y_true_pose], axis=0)\n    y_pred_pose = np.concatenate([t.cpu().numpy() for t in y_pred_pose], axis=0)\n    y_true_pose = y_true_pose.reshape(-1)\n    y_pred_pose = y_pred_pose.reshape(-1)\n\n    # Print the results\n    r2 = r2_score(y_true_pose,y_pred_pose)\n    training_error_pose.append(R_te_errorPose/te_countPose)\n\n    print('No of epochs Trained: ',epoch+1)\n    print('For Gender Task:')\n    gender_accuracy_train.append(calculate_classification_metrics(y_true_gender,y_pred_gender))\n    print('For Glasses Task:')\n    glass_accuracy_train.append(calculate_classification_metrics(y_true_glass,y_pred_glass))\n    print('For Pose Task:')       \n    print(f\"R2 Score: {r2:.4f}\")\n    print('Training error pose: ' + str(R_te_errorPose/te_countPose))\n    multi_task_net.train()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-19T09:47:28.778486Z","iopub.execute_input":"2024-04-19T09:47:28.779120Z","iopub.status.idle":"2024-04-19T09:47:28.795324Z","shell.execute_reply.started":"2024-04-19T09:47:28.779087Z","shell.execute_reply":"2024-04-19T09:47:28.794410Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def validation_result(epoch, multi_task_net, val_dataloader):\n    criterion_task3 = myMSElossPose(batch_size).to(device)\n    multi_task_net.eval()\n\n    y_true_gender, y_pred_gender = [], []\n    y_true_glass, y_pred_glass = [], []\n    y_true_pose, y_pred_pose = [], []\n    R_te_errorPose=0\n    te_countPose=0\n\n\n    with torch.no_grad():\n        for i, data in enumerate(val_dataloader, 0):\n            img, label_gender, label_glass, label_pose, imageName, TaskwiseLabelPresence = data\n            img = img.to(device)\n            label_gender = label_gender.to(device)\n            label_glass = label_glass.to(device)\n            label_pose = label_pose.to(device)\n            TaskwiseLabelPresence = TaskwiseLabelPresence.numpy()\n            img, label_gender, label_glass, label_pose = img.float(), label_gender.float(), label_glass.float(), label_pose.float()\n\n            FC_output_gender, FC_output_glasses, FC_output_angle = multi_task_net(img)\n\n            gender_IndSelect = np.where(TaskwiseLabelPresence[:, 0] == 1)\n            glasses_IndSelect = np.where(TaskwiseLabelPresence[:, 1] == 1)\n            pose_IndSelect = np.where(TaskwiseLabelPresence[:, 2] == 1)\n\n            if gender_IndSelect[0].shape[0] != 0:\n                y_true_gender.append(label_gender)\n                y_pred_gender.append(FC_output_gender)\n\n            if glasses_IndSelect[0].shape[0] != 0:\n                y_true_glass.append(label_glass)\n                y_pred_glass.append(FC_output_glasses)\n\n            if pose_IndSelect[0].shape[0] != 0:\n                y_true_pose.append(label_pose)\n                y_pred_pose.append(FC_output_angle)\n                loss3 = criterion_task3(FC_output_angle, label_pose)\n                R_te_errorPose += loss3.item()\n                te_countPose = te_countPose+1\n\n\n    y_true_gender = np.concatenate([t.cpu().numpy() for t in y_true_gender], axis=0)\n    y_pred_gender = np.concatenate([t.cpu().numpy() for t in y_pred_gender], axis=0)\n    y_true_glass = np.concatenate([t.cpu().numpy() for t in y_true_glass], axis=0)\n    y_pred_glass = np.concatenate([t.cpu().numpy() for t in y_pred_glass], axis=0)\n    y_true_pose = np.concatenate([t.cpu().numpy() for t in y_true_pose], axis=0)\n    y_pred_pose = np.concatenate([t.cpu().numpy() for t in y_pred_pose], axis=0)\n    y_true_pose = y_true_pose.reshape(-1)\n    y_pred_pose = y_pred_pose.reshape(-1)\n\n    # Print the results\n    r2 = r2_score(y_true_pose,y_pred_pose)\n    val_error_pose.append(R_te_errorPose/te_countPose)\n\n    print('No of epochs Trained: ',epoch+1)\n    print('For Gender Task:')\n    gender_accuracy_val.append(calculate_classification_metrics(y_true_gender,y_pred_gender))\n    print('For Glasses Task:')\n    glass_accuracy_val.append(calculate_classification_metrics(y_true_glass,y_pred_glass))\n    print('For Pose Task:')       \n    print(f\"R2 Score: {r2:.4f}\")\n    print('Training error pose: ' + str(R_te_errorPose/te_countPose))\n    multi_task_net.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-19T09:47:30.351411Z","iopub.execute_input":"2024-04-19T09:47:30.351803Z","iopub.status.idle":"2024-04-19T09:47:30.370458Z","shell.execute_reply.started":"2024-04-19T09:47:30.351770Z","shell.execute_reply":"2024-04-19T09:47:30.369455Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def testing_result(epoch, multi_task_net, test_dataloader):\n    \n        criterion_task3 = myMSElossPose(batch_size).to(device)\n        multi_task_net.eval()\n\n        y_true_glass = []\n        y_pred_glass = []\n        y_true_gender = []\n        y_pred_gender = []\n        y_true_pose = []\n        y_pred_pose = []\n        R_te_errorPose=0\n        te_countPose=0\n\n\n        with torch.no_grad():\n            for i, data in enumerate(test_dataloader, 0):\n                img, label_gender, label_glass, label_pose, imageName = data\n                img = img.to(device)\n                label_gender = label_gender.to(device)\n                label_glass = label_glass.to(device)\n                label_pose = label_pose.to(device)\n                img, label_gender, label_glass, label_pose = img.float(), label_gender.float(), label_glass.float(), label_pose.float()\n\n                # Make predictions\n                FC_output_gender, FC_output_glasses, FC_output_angle = multi_task_net(img)\n\n\n                # Calculate task-wise metrics\n                y_true_gender.append(label_gender)\n                y_pred_gender.append(FC_output_gender)\n\n                y_true_glass.append(label_glass)\n                y_pred_glass.append(FC_output_glasses)\n\n                y_true_pose.append(label_pose)\n                y_pred_pose.append(FC_output_angle)\n                loss3 = criterion_task3(FC_output_angle, label_pose)\n                R_te_errorPose += loss3.item()\n                te_countPose = te_countPose+1\n\n        \n        y_true_gender = np.concatenate([t.cpu().numpy() for t in y_true_gender], axis=0)\n        y_pred_gender = np.concatenate([t.cpu().numpy() for t in y_pred_gender], axis=0)\n        y_true_glass = np.concatenate([t.cpu().numpy() for t in y_true_glass], axis=0)\n        y_pred_glass = np.concatenate([t.cpu().numpy() for t in y_pred_glass], axis=0)\n        y_true_pose = np.concatenate([t.cpu().numpy() for t in y_true_pose], axis=0)\n        y_pred_pose = np.concatenate([t.cpu().numpy() for t in y_pred_pose], axis=0)\n        y_true_pose = y_true_pose.reshape(-1)\n        y_pred_pose = y_pred_pose.reshape(-1)\n\n        r2 = r2_score(y_true_pose,y_pred_pose)\n        testing_error_pose.append(R_te_errorPose/te_countPose)\n\n        # Print the results\n        print('No of epochs Trained: ',epoch+1)\n        print('For Gender Task:')\n        gender_accuracy_test.append(calculate_classification_metrics(y_true_gender,y_pred_gender))\n        print('For Glasses Task:')\n        glass_accuracy_test.append(calculate_classification_metrics(y_true_glass,y_pred_glass))\n        print('For Pose Task:')       \n        print(f\"R2 Score: {r2:.4f}\")\n        print('Testing error pose: ' + str(R_te_errorPose/te_countPose))\n        multi_task_net.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-19T09:47:45.865837Z","iopub.execute_input":"2024-04-19T09:47:45.866668Z","iopub.status.idle":"2024-04-19T09:47:45.881235Z","shell.execute_reply.started":"2024-04-19T09:47:45.866636Z","shell.execute_reply":"2024-04-19T09:47:45.880218Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"for epoch in range(NumEpochs):\n    print(\"Epoch: %d\" % epoch)\n    rloss1 = 0.0\n    rloss2 = 0.0\n    rloss3 = 0.0\n    rnet_loss = 0.0\n\n    multi_task_net.train()\n    for i, data in enumerate(train_dataloader, 0):\n        img, label_gender, label_glass, label_pose, imageName, TaskwiseLabelPresence = data\n        img = img.to(device)\n        label_gender = label_gender.to(device)\n        label_glass = label_glass.to(device)\n        label_pose = label_pose.to(device)\n        TaskwiseLabelPresence = TaskwiseLabelPresence.numpy()\n      \n        img, label_gender, label_glass, label_pose = img.float(), label_gender.float(), label_glass.float(), label_pose.float()\n\n        # Zero the gradients\n        optimizer.zero_grad()\n\n        FC_output_gender, FC_output_glasses, FC_output_angle = multi_task_net(img)\n\n        gender_IndSelect = np.where(TaskwiseLabelPresence[:, 0] == 1)\n        glasses_IndSelect = np.where(TaskwiseLabelPresence[:, 1] == 1)\n        pose_IndSelect = np.where(TaskwiseLabelPresence[:, 2] == 1)\n\n        if gender_IndSelect[0].shape[0] != 0:\n            loss1 = criterion_task1(FC_output_gender[gender_IndSelect[0]], label_gender[gender_IndSelect[0]])\n            rloss1 += loss1.item()\n        else:\n            loss1 = 0.0\n\n        if glasses_IndSelect[0].shape[0] != 0:\n            loss2 = criterion_task2(FC_output_glasses[glasses_IndSelect], label_glass[glasses_IndSelect])\n            rloss2 += loss2.item()\n        else:\n            loss2 = 0.0\n\n        if pose_IndSelect[0].shape[0] != 0:\n            loss3 = criterion_task3(FC_output_angle[pose_IndSelect[0]], label_pose[pose_IndSelect[0]])\n            rloss3 += loss3.item()\n        else:\n            loss3 = 0.0\n\n        NetLoss = loss1 + loss2 + loss3\n\n        if epoch != 0:\n            NetLoss.backward()\n            optimizer.step()\n\n        rnet_loss += NetLoss.item()\n\n    print(\"Task1 Loss: {:.4f}, Task2 Loss: {:.4f}, Task3 Loss: {:.4f}, Total Loss: {:.4f}\".format(\n        rloss1 / len(train_dataloader),\n        rloss2 / len(train_dataloader),\n        rloss3 / len(train_dataloader),\n        rnet_loss / len(train_dataloader)\n    ))\n\n\n    multi_task_net.eval()\n    with torch.no_grad():\n        val_loss = 0.0\n        for i, data in enumerate(val_dataloader, 0):\n            img, label_gender, label_glass, label_pose, imageName, TaskwiseLabelPresence = data\n            img = img.to(device)\n            label_gender = label_gender.to(device)\n            label_glass = label_glass.to(device)\n            label_pose = label_pose.to(device)\n            TaskwiseLabelPresence = TaskwiseLabelPresence.numpy()\n            img, label_gender, label_glass, label_pose = img.float(), label_gender.float(), label_glass.float(), label_pose.float()\n\n            FC_output_gender, FC_output_glasses, FC_output_angle = multi_task_net(img)\n\n            gender_IndSelect = np.where(TaskwiseLabelPresence[:, 0] == 1)\n            glasses_IndSelect = np.where(TaskwiseLabelPresence[:, 1] == 1)\n            pose_IndSelect = np.where(TaskwiseLabelPresence[:, 2] == 1)\n\n            loss1 = criterion_task1(FC_output_gender[gender_IndSelect[0]], label_gender[gender_IndSelect[0]]) if gender_IndSelect[0].shape[0] != 0 else 0.0\n            loss2 = criterion_task2(FC_output_glasses[glasses_IndSelect], label_glass[glasses_IndSelect]) if glasses_IndSelect[0].shape[0] != 0 else 0.0\n            loss3 = criterion_task3(FC_output_angle[pose_IndSelect[0]], label_pose[pose_IndSelect[0]]) if pose_IndSelect[0].shape[0] != 0 else 0.0\n\n\n            val_loss += (loss1 + loss2 + loss3).item()\n\n\n    val_loss /= len(val_dataloader)\n\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        best_model_state = multi_task_net.state_dict()\n        patience = 0\n    else:\n        patience += 1\n        if patience >= early_stopping_rounds:\n            print(\"Early stopping triggered.\")\n            break\n\n\n    print('Training Result')\n    training_result(epoch,multi_task_net, train_dataloader)\n    print('Validation Result')\n    validation_result(epoch,multi_task_net, val_dataloader)\n    print('Testing Result')\n    testing_result(epoch,multi_task_net, test_dataloader)\n\n\n    multi_task_net.load_state_dict(best_model_state)\n    torch.save(multi_task_net, 'Teacher_MTL.pth')","metadata":{"execution":{"iopub.status.busy":"2024-04-19T09:47:50.738795Z","iopub.execute_input":"2024-04-19T09:47:50.739133Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch: 0\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}